{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j3Inuef5NVHd",
      "metadata": {
        "id": "j3Inuef5NVHd"
      },
      "outputs": [],
      "source": [
        "!pip install torch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "417938cd-285d-468c-9352-f5298c3fd75c",
      "metadata": {
        "id": "417938cd-285d-468c-9352-f5298c3fd75c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torchsummary import summary\n",
        "# import torch_optimizer as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "J2w-XLSSG7LE",
      "metadata": {
        "id": "J2w-XLSSG7LE"
      },
      "outputs": [],
      "source": [
        "# Function for data augmentation for train and test dataset\n",
        "def get_transformations():\n",
        "    train_transform = transforms.Compose([\n",
        "        # transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32, padding=6), # Randomly crops the image and resizes it. If the crop image size is same as the image, it randomizes the image patches\n",
        "        transforms.RandomHorizontalFlip(), # Horizontally flips the image with the probability of 50%\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # increases brightness, contrast, sturation and hue of the image\n",
        "        transforms.RandomRotation(20), # Randomly rotates the image with degree 20\n",
        "        transforms.RandomAffine(degrees=10, translate=(0.2, 0.2)), # Applies random affine\n",
        "        transforms.ToTensor(), # Converts the image to tensors, also scales it from 0-1\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)), # Normalize\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        # transforms.ToPILImage(),\n",
        "        transforms.ToTensor(), # Converts the image to tensors, scales it from 0-1\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)), # Normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, test_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b5d9752-0e48-46ba-bd1e-8bcde83f0c64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b5d9752-0e48-46ba-bd1e-8bcde83f0c64",
        "outputId": "b325b820-e196-431e-fb49-736eddf5200f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 33.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Dataset\n",
        "train_transform, test_transform = get_transformations()\n",
        "train = datasets.CIFAR10('./data', train=True, download=True, transform=train_transform)\n",
        "test = datasets.CIFAR10('./data', train=False, download=True, transform=test_transform)\n",
        "# Data Loader\n",
        "train_data_loader  = torch.utils.data.DataLoader(train, batch_size=128,shuffle=True)\n",
        "validation_data_loader  = torch.utils.data.DataLoader(test, batch_size=128,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d1f936fe-ea50-445e-a4d4-b003449b1c14",
      "metadata": {
        "id": "d1f936fe-ea50-445e-a4d4-b003449b1c14"
      },
      "outputs": [],
      "source": [
        "# ResNet Model\n",
        "\n",
        "# Squeeze and excitation block\n",
        "class SE_Block(torch.nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SE_Block, self).__init__()\n",
        "        # Adaptive Average Pooling:\n",
        "        # This reduces the spatial dimensions (H x W) to 1x1, effectively summarizing\n",
        "        # the feature maps across spatial dimensions into a single value per channel.\n",
        "        self.avg_pool = torch.nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Fully connected layers for recalibrating channel importance\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(channels, channels // reduction, bias=False),  # Reduction step: Compress the channel dimension\n",
        "            torch.nn.ReLU(inplace=True),  # Non-linearity to introduce non-linear transformations\n",
        "            torch.nn.Linear(channels // reduction, channels, bias=False),  # Restore the channel dimension\n",
        "            torch.nn.Sigmoid()  # Outputs attention weights in the range [0,1] to reweight feature maps\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        batch, channels, w, h = x.size() # Extract batch size, channel, width, and height\n",
        "\n",
        "        # Squeeze step: Apply global average pooling and reshape to (batch, channels)\n",
        "        y = self.avg_pool(x).view(batch, channels)\n",
        "\n",
        "        # Excitation step: Pass through the fully connected layers to learn channel importance\n",
        "        y = self.fc(y).view(batch, channels, 1, 1) # Reshape to (batch, channels, 1, 1) for broadcasting\n",
        "\n",
        "        # Scale the original input feature maps by learned channel-wise weights\n",
        "        return x * y.expand_as(x)# Broadcasting ensures each channel is multiplied by its learned weight\n",
        "\n",
        "class ResNet_Block(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, use_se_block=False):\n",
        "        super(ResNet_Block, self).__init__()\n",
        "\n",
        "        # Architechture (Basic)\n",
        "        # CONV -> BN -> ReLU -> CONV -> BN -> Addition -> ReLU\n",
        "        # Architechture (With SE)\n",
        "        # CONV -> BN -> ReLU -> CONV -> BN -> SE Block -> Addition -> ReLU\n",
        "\n",
        "        # First convolutional layer:\n",
        "        # - Uses a 3x3 kernel with possible downsampling (stride != 1 for size reduction)\n",
        "        # - Applies Batch Normalization to stabilize training\n",
        "        # - Uses ReLU activation for non-linearity\n",
        "        self.convolution1 = torch.nn.Sequential(\n",
        "                                torch.nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding=1), # Here the stride is not 1 because we might be coming from a bigger image size and we might need to downsample\n",
        "                                torch.nn.BatchNorm2d(out_channels),\n",
        "                                torch.nn.ReLU())\n",
        "        # Second convolutional layer:\n",
        "        # - Uses a 3x3 kernel with a fixed stride of 1 (no downsampling here)\n",
        "        # - Applies Batch Normalization\n",
        "        # - No activation function applied before the skip connection addition\n",
        "        self.convolution2 = torch.nn.Sequential(\n",
        "                                torch.nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1), # Here the stride is always 1 because this is the second conv layer is not downsampled\n",
        "                                torch.nn.BatchNorm2d(out_channels))\n",
        "        # Downsampling layer:\n",
        "        # - Used when the input and output dimensions differ (e.g., due to stride > 1)\n",
        "        # - Ensures the skip connection can match the new feature map dimensions\n",
        "        self.downsample = downsample\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        # Option to use a Squeeze-and-Excitation (SE) block\n",
        "        self.use_se_block = use_se_block\n",
        "        self.se_block = SE_Block(out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x # Residual\n",
        "        if self.downsample is not None: # If downsampling\n",
        "            res = self.downsample(x) # The input is downsampled\n",
        "\n",
        "        out = self.convolution1(x) # Conv1\n",
        "        out = self.convolution2(out) # Conv2\n",
        "\n",
        "        #SE Block\n",
        "        if self.use_se_block:\n",
        "          out = self.se_block(out)\n",
        "\n",
        "        z = out + res # Identity mapping\n",
        "        z = self.relu(z) # Final ReLU\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "class PreActivation_ResNet_Block(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, use_se_block=False):\n",
        "        super(PreActivation_ResNet_Block, self).__init__()\n",
        "\n",
        "        # Architechture (Basic)\n",
        "        # BN -> ReLU -> CONV ->  BN -> ReLU -> CONV -> Addition\n",
        "        # Architechture (With SE)\n",
        "        # BN -> ReLU -> CONV ->  BN -> ReLU -> CONV -> SE Block -> Addition\n",
        "\n",
        "        # Batch Normalization\n",
        "        self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # ReLU\n",
        "        self.relu1 = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        #Uses a 3x3 kernel with possible downsampling (stride != 1 for size reduction)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "        # Batch Normalization\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # ReLU\n",
        "        self.relu2 = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        #Uses a 3x3 kernel with a fixed stride of 1 (no downsampling here)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.downsample = downsample\n",
        "        self.use_se_block = use_se_block\n",
        "        self.se_block = SE_Block(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(out)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        if self.use_se_block:\n",
        "          out = self.se_block(out)\n",
        "\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.block = block\n",
        "        # Input = 3 channels, 32 * 32\n",
        "        # Initial Convolution that uses a 5 * 5 kernel with stride = 1 and padding = 2.\n",
        "        # Creates 64 channels\n",
        "        # Batch Normalization\n",
        "        # ReLU\n",
        "        self.convolution1 = torch.nn.Sequential(\n",
        "                                torch.nn.Conv2d(3, 64, kernel_size = 5, stride = 1, padding = 2),\n",
        "                                torch.nn.BatchNorm2d(64),\n",
        "                                torch.nn.ReLU()) # Output size => 32*32\n",
        "        # First ResNet Block with input 64 channels and output 64 channels. Image size does not change\n",
        "        self.layer0 = self.add_res_net_block(64, 64, layers[0], first_layer_stride = 1) # 32*32\n",
        "\n",
        "        # Second ResNet Block with input 64 channels and output 128 channels. Image size = 16 * 16\n",
        "        self.layer1 = self.add_res_net_block(64, 128, layers[1], first_layer_stride = 2)# 16*16\n",
        "\n",
        "         # Second ResNet Block with input 128 channels and output 256 channels. Image size = 8 * 8\n",
        "        self.layer2 = self.add_res_net_block(128, 256, layers[2], first_layer_stride = 2)#8*8\n",
        "\n",
        "        # Average pool. Reduces the spatial dimensions of every channel to 1 node\n",
        "        self.avgpool = torch.nn.AvgPool2d(8, stride=1)\n",
        "\n",
        "        # Final connected layer with 10 output\n",
        "        self.fc = torch.nn.Linear(256, 10)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d): # Initialize from kaming normal, if it is a convolution layer.\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, torch.nn.BatchNorm2d): # If batch normalization ==> weights = 1 and biases = 0\n",
        "                torch.nn.init.constant_(m.weight, 1)\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Function to add a block\n",
        "    '''\n",
        "        Arguments\n",
        "            Input channels\n",
        "            output channels\n",
        "            number of layers\n",
        "            initial stride (to check for downsampling)\n",
        "    '''\n",
        "    def add_res_net_block(self, in_channels, out_channels, layers, first_layer_stride):\n",
        "        downsample = None\n",
        "        num_layers, use_se_block = layers # un pack layers (num_layers:int, use_se_block:bool)\n",
        "\n",
        "        # Check if downsampling is needed:\n",
        "        # - If stride > 1, the spatial dimensions will shrink, requiring downsampling.\n",
        "        # - If input and output channels differ, a 1x1 convolution is used to match dimensions.\n",
        "        if first_layer_stride != 1 or in_channels != out_channels:\n",
        "\n",
        "            downsample = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride=first_layer_stride),\n",
        "                torch.nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        block_layers = []\n",
        "        # First layer of the residual block:\n",
        "        # - Uses downsampling if needed\n",
        "        block_layers.append(self.block(in_channels, out_channels, first_layer_stride, downsample))\n",
        "        for i in range(num_layers-1):\n",
        "            # Remaining layers of the residual block:\n",
        "            # - Stride is always 1 (no further downsampling)\n",
        "            # - Uses SE block if enabled\n",
        "            if use_se_block:\n",
        "                block_layers.append(self.block(out_channels, out_channels, 1, None, True))\n",
        "            else:\n",
        "                block_layers.append(self.block(out_channels, out_channels, 1, None, False))\n",
        "        return torch.nn.Sequential(*block_layers) # return sequence of all the layers as a block\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution1(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "omfB6nvOdEu0",
      "metadata": {
        "id": "omfB6nvOdEu0"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "'''\n",
        "    Arguments:\n",
        "        model\n",
        "        train_data_loader\n",
        "        validation_data_loader\n",
        "        label_smoothing\n",
        "        learning rate\n",
        "        weight decay\n",
        "        momentum\n",
        "        nesterov\n",
        "        lookahead\n",
        "        epochs\n",
        "\n",
        "    Returns:\n",
        "        Trained model\n",
        "        train Loss\n",
        "        validation Loss\n",
        "        train accuracy\n",
        "        validation accuracy\n",
        "'''\n",
        "def train_model(model, train_data_loader, validation_data_loader, label_smoothing = 0.1, lr = 0.1, weight_decay = 0.0005, momentum=0.9, nesterov=True, lookahead=False, epochs=100):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Get the device\n",
        "    loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing) # Loss function\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = lr, weight_decay = weight_decay, momentum=momentum, nesterov=nesterov) # SGD optimizer with initial learning rate, weight decay, momentum, and nesterov\n",
        "    if lookahead: # if lookahead is selected the update the optimizer\n",
        "        optimizer = optim.Lookahead(optimizer, k=5, alpha=0.5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.0001) # scheduler\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        trainloss = 0.0\n",
        "        valloss = 0.0\n",
        "        val_correct = 0\n",
        "        train_correct = 0\n",
        "        val_total = 0\n",
        "        train_total = 0\n",
        "\n",
        "        model.train() # telling python that we are intereseted in updating any trainable parameters in the network\n",
        "        for images, labels in train_data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad() # makes sure we have zeroes out gradients for trainable parameters from the previous iteration\n",
        "            pred = model(images) # forward pass\n",
        "            fit = loss(pred, labels) # Calculate loss\n",
        "            fit.backward() # backpropogation\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # Clipping gradients to norm value of 1 --> [-1, 1]\n",
        "            optimizer.step() # updates the weight\n",
        "            trainloss += fit.item()\n",
        "            _, predicted = torch.max(pred, 1) # Get indexes of the most confident guess\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        model.eval() # Switch the model to evaluate mode.\n",
        "        for images, labels in validation_data_loader:\n",
        "            with torch.no_grad(): # Makes sure that gradient calculation is disabled\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                pred = model(images) # forward pass\n",
        "                fit = loss(pred, labels) # calculate loos\n",
        "                valloss += fit.item()\n",
        "                _, predicted = torch.max(pred, 1) # Get indexes of the most confident guess\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        trainloss = trainloss/len(train_data_loader)\n",
        "        valloss = valloss/len(validation_data_loader)\n",
        "        val_loss.append(valloss)\n",
        "        train_loss.append(trainloss)\n",
        "\n",
        "        val_accuracy = 100 * val_correct/val_total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        train_accuracy = 100 * train_correct/train_total\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "        scheduler.step() # Update the learning rate\n",
        "\n",
        "        print(f'Epoch: {epoch+1}/{epochs} | Train Loss: {trainloss:.2f} | val loss: {valloss:.2f} | val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    return model, train_loss, val_loss, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "xDPcQnvMICLX",
      "metadata": {
        "id": "xDPcQnvMICLX"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Here layers are [Block, Block... <Number of blocks>], Block = Tuple(<number of layers>, <should this layer have SE block>)\n",
        "layers = [(4, True),(4, True),(3, True)]\n",
        "# ResNet model takes 2 parameters ==> What block to use and layers\n",
        "model = ResNet(ResNet_Block, layers).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3UETng8sIKBG",
      "metadata": {
        "id": "3UETng8sIKBG"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model, train_loss, val_loss, train_accuracies, val_accuracies = train_model(model, train_data_loader, validation_data_loader,label_smoothing = 0.1, lr = 0.1, weight_decay = 0.0005, momentum=0.9, nesterov=True, lookahead=False, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734700fb-ce69-4af6-8eaa-49fa5c461716",
      "metadata": {
        "id": "734700fb-ce69-4af6-8eaa-49fa5c461716"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
        "ax[0].plot(train_loss, label=\"Train Loss\")\n",
        "ax[0].plot(val_loss, label=\"Validation Loss\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_accuracies, label=\"Train Accuracy\")\n",
        "ax[1].plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "ax[1].legend()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TmgPkyTAJYAT",
      "metadata": {
        "id": "TmgPkyTAJYAT"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), './ResNet4_4_3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "klXjmlS6s-m-",
      "metadata": {
        "id": "klXjmlS6s-m-"
      },
      "outputs": [],
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bIW_L8Hr2Q5q",
      "metadata": {
        "id": "bIW_L8Hr2Q5q"
      },
      "outputs": [],
      "source": [
        "pred_batch = unpickle('cifar_test_nolabel.pkl')\n",
        "pred_images = pred_batch[b'data']\n",
        "pred_dataset = [test_transform(img) for img in pred_images]\n",
        "pred_loader =  torch.utils.data.DataLoader(pred_dataset, batch_size=128, shuffle = False)\n",
        "model.eval()\n",
        "predictions_made = []\n",
        "with torch.no_grad():\n",
        "  for images in pred_loader:\n",
        "    images = images.to(device)\n",
        "    pred = model(images)\n",
        "    _, predicted = torch.max(pred, 1)\n",
        "    predictions_made.extend(predicted.cpu().numpy())\n",
        "benchmark = pd.DataFrame({'ID': [i for i in range(len(predictions_made))], 'Labels': predictions_made})\n",
        "benchmark.to_csv('./benchmark.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
